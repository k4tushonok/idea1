{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888d46b3",
   "metadata": {},
   "source": [
    "## Импорты и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e8faee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:33:53.057749Z",
     "start_time": "2026-01-27T19:33:39.064593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Caskroom/miniforge/base/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 602, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8z/453bprns0jqfklvcj_jfz8hr0000gn/T/ipykernel_79653/1934134305.py\", line 5, in <module>\n",
      "    from datasets import load_dataset\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/datasets/__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Column, Dataset\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 104, in <module>\n",
      "    from .formatting import format_table, get_format_type_from_alias, get_formatter, query_table\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/datasets/formatting/__init__.py\", line 91, in <module>\n",
      "    from .torch_formatter import TorchFormatter\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py\", line 32, in <module>\n",
      "    import torch\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/kateaver/idea1/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from data_structures import Example\n",
    "from hierarchical_optimizer import HierarchicalOptimizer\n",
    "\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3b48c96eb8925",
   "metadata": {},
   "source": [
    "### Функции для подготовки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67498d6ffc317f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:33:53.081651Z",
     "start_time": "2026-01-27T19:33:53.066015Z"
    }
   },
   "outputs": [],
   "source": [
    "def squad_v2_to_examples(data):\n",
    "    examples = []\n",
    "    for item in data:\n",
    "        input_text = (f'# Question: \\n {item[\"question\"]} \\n'\n",
    "                      f'# Context: \\n {item[\"context\"]} \\n')\n",
    "        expected_output = item['answers']['text'][0] if item['answers']['text'] else 'No answer'\n",
    "        examples.append(Example(input_text=input_text, expected_output=expected_output))\n",
    "    return examples\n",
    "\n",
    "def get_squad_v2_data(train_num: int, val_ratio: float, test_num: int):\n",
    "    ds_train = load_dataset('rajpurkar/squad_v2', split='train')\n",
    "    ds_test = load_dataset('rajpurkar/squad_v2', split='validation')\n",
    "\n",
    "    split = ds_train.train_test_split(test_size=val_ratio)\n",
    "    ds_train = split['train']\n",
    "    ds_val = split['test']\n",
    "\n",
    "    ds_train = ds_train.shuffle()\n",
    "    ds_val = ds_val.shuffle()\n",
    "    ds_test = ds_test.shuffle()\n",
    "\n",
    "    train_split = ds_train.select(range(train_num))\n",
    "    val_split = ds_val.select(range(int(train_num * val_ratio)))\n",
    "    test_split = ds_test.select(range(test_num))\n",
    "\n",
    "    train_examples = squad_v2_to_examples(train_split)\n",
    "    validation_examples = squad_v2_to_examples(val_split)\n",
    "    test_examples = squad_v2_to_examples(test_split)\n",
    "\n",
    "    return train_examples, validation_examples, test_examples\n",
    "\n",
    "def data_fabric(dataset: str = 'squad_v2', train_num: int = 200, val_ratio: float = 0.2, test_num: int = 1000):\n",
    "    squad_v2_initial_prompt = \"\"\"Answer the question based on the context. If there is no answer in the context then just return 'No answer'.\\n\"\"\"\n",
    "\n",
    "    train_examples, validation_examples, test_examples = get_squad_v2_data(train_num, val_ratio, test_num)\n",
    "    initial_prompt = squad_v2_initial_prompt\n",
    "    \n",
    "    return train_examples, validation_examples, test_examples, initial_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fea28",
   "metadata": {},
   "source": [
    "## Подготовка датасета\n",
    "Создаем простой датасет для демонстрации (задача классификации тональности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00eaecc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:33:57.651301Z",
     "start_time": "2026-01-27T19:33:53.081651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared:\n",
      "  Train: 200 examples\n",
      "  Validation: 40 examples\n",
      "  Test: 1000 examples\n"
     ]
    }
   ],
   "source": [
    "LABEL_MAP = {0: \"truth\", 1: \"lie\"}\n",
    "\n",
    "train_examples, validation_examples, test_examples, initial_prompt = data_fabric('squad_v2')\n",
    "\n",
    "print(\"Dataset prepared:\")\n",
    "print(f\"  Train: {len(train_examples)} examples\")\n",
    "print(f\"  Validation: {len(validation_examples)} examples\")\n",
    "print(f\"  Test: {len(test_examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506755da",
   "metadata": {},
   "source": [
    "## Создание начального промпта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9f5e09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:33:57.663249Z",
     "start_time": "2026-01-27T19:33:57.657677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "------------------------------------------------------------\n",
      "Answer the question based on the context. If there is no answer in the context then just return 'No answer'.\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial prompt:\")\n",
    "print(\"-\" * 60)\n",
    "print(initial_prompt)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8902c14",
   "metadata": {},
   "source": [
    "## Инициализация оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7384e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T19:34:06.575300Z",
     "start_time": "2026-01-27T19:33:57.692096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "optimizer = HierarchicalOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d6ed0",
   "metadata": {},
   "source": [
    "## Запуск оптимизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30001454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:13:41.359526Z",
     "start_time": "2026-01-27T19:34:06.595445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating initial prompt...\n",
      "Initial score: 0.731\n",
      "  Accuracy: 0.700\n",
      "  Safety: 0.500\n",
      "  Robustness: 0.925\n",
      "  Efficiency: 0.767\n",
      "  F1: 0.762\n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERATION 1/3\n",
      "================================================================================\n",
      "\n",
      "Phase 1: Local Optimization\n",
      "  Population size: 1\n",
      "\n",
      "  Optimizing node 1/1 (score: 0.731)\n",
      "\n",
      "============================================================\n",
      "Starting Local Optimization\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Failures: 6, Successes: 24\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'all': 6 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 2 gradients\n",
      "  Generating variants from gradient 1/2\n",
      "  Generating variants from gradient 2/2\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To rate the similarity between these two prompts, let\\'s break down their components:\\n\\n**Intent:**\\n- **Prompt A:** Directly states that the context should be considered irrelevant or misleading and suggests asking a follow-up question or changing the topic.\\n- **Prompt B:** Recommends returning \"No answer\" when the context lacks direct information but encourages continuing analysis for a more likely correct answer.\\n\\nBoth prompts aim to handle cases where the provided context doesn\\'t provide sufficient information, but they approach it differently. However, they both have similar core intents.\\n\\n**Constraints:**\\n- Both prompts suggest handling incomplete or irrelevant contexts by either asking a follow-up question or returning \"No answer.\"\\n\\nWhile there are slight differences in wording, the overall constraint remains consistent across both prompts.\\n\\n**Structure:**\\n- **Prompt A:** Uses straightforward language with specific instructions (\"Consider\", \"Ask\").\\n- **Prompt B:** Also uses clear directives (\"Continue analyzing\"), though its phrasing might be slightly less explicit.\\n\\nThe structure is quite similar; both use conditional statements about context quality affecting the response.\\n\\n**Examples and Formatting:**\\n- **Prompt A:** Explicit directive with no extra formatting.\\n- **Prompt B:** Suggests using \"No answer,\" which could imply some formatting (e.g., markdown), but this is minimal compared to Prompt A.\\n\\nOverall, while there are minor variations in wording and formatting, the fundamental purpose and approach remain very similar.\\n\\n**Final Rating:**\\nMy assessment indicates a moderate level of similarity due to shared core intentions and constraints, combined with slight structural differences. Therefore, I would rate the similarity at approximately **0.75** out of 1.0.\\n\\nThis rating reflects the fact that despite small differences in phrasing and format, both prompts serve the same general purpose and methodology.'\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To rate the similarity of these two prompts, let\\'s break down their key elements:\\n\\n**Intent:**\\n- **Prompt A:** Directly asks for alternative questions based on the relevance/irrelevance of the context.\\n- **Prompt B:** Requests that if the context isn\\'t sufficient, it should return \"No answer.\" Otherwise, it analyzes the context to find the best possible answer.\\n\\nSimilarity: 0.8 (because both prompts aim to handle situations where no direct answer can be given)\\n\\n**Constraints:**\\n- Both prompts have similar constraints regarding when to return \"No answer.\"\\n- However, Prompt A provides more specific guidance on what actions to take if the context is deemed irrelevant or misleading.\\n\\nSimilarity: 0.75\\n\\n**Structure:**\\n- **Prompt A:** Explicit instructions followed by two potential responses.\\n- **Prompt B:** More general advice with a conditional statement (\"otherwise\").\\n\\nSimilarity: 0.6\\n\\n**Examples and Formatting:**\\n- **Prompt A:** Uses numbered options.\\n- **Prompt B:** Provides a simple text response.\\n\\nSimilarity: 0.4\\n\\nOverall rating: 0.6\\n\\nThe prompts share some similarities but differ significantly in their approach to handling ambiguous contexts. Therefore, they score slightly below a perfect 1.0 due to the nuanced differences in their instructions and overall structure.'\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: \"To rate the similarity between these two prompts, let's break down their key components:\\n\\n### Intent\\n- **Prompt A:** The intent is to suggest either a follow-up question or a new question based on relevance and helpfulness.\\n- **Prompt B:** The intent is also to provide guidance on whether the given context is relevant or misleading and what alternative actions should be taken (asking a follow-up question or suggesting a different topic).\\n\\n**Similarity:**\\nBoth prompts aim to provide feedback on the usefulness and relevance of the context. Therefore, they have a high degree of similarity in intent.\\n\\n### Constraints\\n- **Prompt A:** No specific constraints mentioned.\\n- **Prompt B:** Assumes that the context is potentially misleading or irrelevant, so it offers more structured advice.\\n\\n**Similarity:**\\nWhile both prompts do not specify strict constraints, Prompt B implies a level of critical thinking about the context's suitability for answering questions, which is somewhat implied but not explicitly stated in Prompt A.\\n\\n### Structure\\n- **Prompt A:** Structured with clear options and instructions.\\n- **Prompt B:** More open-ended, guiding towards specific actions without explicit instructions.\\n\\n**Similarity:**\\nThe structure of Prompt B leans more towards providing direct responses rather than offering choices, making them slightly less similar structurally.\\n\\n### Examples and Formatting\\n- **Prompt A:** Uses bullet points and numbered options.\\n- **Prompt B:** Directly states possible answers without additional formatting.\\n\\n**Similarity:**\\nBoth prompts use simple, straightforward language, but the overall style differs slightly due to the detailed nature of Prompt A.\\n\\n### Final Rating\\nGiven the similarities in intent and general approach, we can rate these prompts as quite close. They share a common goal but differ in specificity and detail.\\n\\n**Rating: 0.95**\\n\\nThis rating reflects the significant overlap in purpose and intent while acknowledging the slight differences in format and detail.\"\n",
      "  Generated 6 variants, 6 unique\n",
      "Generated 6 candidate prompts\n",
      "  Evaluating candidate 1/6... Score: 0.461\n",
      "  Evaluating candidate 2/6... Score: 0.729\n",
      "  Evaluating candidate 3/6... Score: 0.635\n",
      "  Evaluating candidate 4/6... Score: 0.501\n",
      "  Evaluating candidate 5/6... Score: 0.845\n",
      "  Evaluating candidate 6/6... Score: 0.657\n",
      "Evaluated 6 candidates\n",
      "Best candidate score: 0.845 (Δ +0.114)\n",
      "✓ Improvement found! New best: 0.845\n",
      "\n",
      "Validation Set Evaluation:\n",
      "  Validation score: 0.817\n",
      "  Validation accuracy: 0.767\n",
      "  Validation f1: 0.804\n",
      "  Validation robustness: 0.883\n",
      "  Validation efficiency: 0.867\n",
      "  Validation safety: 0.767\n",
      "Iteration time: 3254.24s — LLM calls: 1441 (total: 1643)\n",
      "\n",
      "--- Iteration 2 ---\n",
      "Failures: 18, Successes: 12\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'all': 18 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 2 gradients\n",
      "  Generating variants from gradient 1/2\n",
      "  Generating variants from gradient 2/2\n",
      "  Generated 0 variants, 0 unique\n",
      "Generated 0 candidate prompts\n",
      "Evaluated 0 candidates\n",
      "✗ No valid candidates generated\n",
      "Iteration time: 388.29s — LLM calls: 5 (total: 1648)\n",
      "\n",
      "--- Iteration 3 ---\n",
      "Failures: 18, Successes: 12\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'all': 18 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 2 gradients\n",
      "  Generating variants from gradient 1/2\n",
      "  Generating variants from gradient 2/2\n",
      "  Generated 5 variants, 5 unique\n",
      "Generated 5 candidate prompts\n",
      "  Evaluating candidate 1/5... Score: 0.676\n",
      "  Evaluating candidate 2/5... Score: 0.831\n",
      "  Evaluating candidate 3/5... Score: 0.748\n",
      "  Evaluating candidate 4/5... Score: 0.694\n",
      "  Evaluating candidate 5/5... Score: 0.709\n",
      "Evaluated 5 candidates\n",
      "Best candidate score: 0.831 (Δ -0.014)\n",
      "✗ No significant improvement\n",
      "Iteration time: 2768.59s — LLM calls: 1063 (total: 2711)\n",
      "\n",
      "Early stopping after 2 iterations without improvement\n",
      "\n",
      "============================================================\n",
      "Local Optimization Complete\n",
      "Final score: 0.845\n",
      "Improvements: 1\n",
      "============================================================\n",
      "\n",
      "\n",
      "Phase 2: Global Optimization (Skipped)\n",
      "\n",
      "Phase 3: Population Update\n",
      "\n",
      "  Generation best: 0.845\n",
      "  Overall best: 0.731\n",
      "  ✓ Improvement: +0.114\n",
      "\n",
      "  Generation time: 6417.51s\n",
      "\n",
      "================================================================================\n",
      "GENERATION 2/3\n",
      "================================================================================\n",
      "\n",
      "Phase 1: Local Optimization\n",
      "  Population size: 1\n",
      "\n",
      "  Optimizing node 1/1 (score: 0.845)\n",
      "\n",
      "============================================================\n",
      "Starting Local Optimization\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Failures: 18, Successes: 12\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'missing_information': 4 failures\n",
      "  Cluster 'incorrect_format': 4 failures\n",
      "  Cluster 'logic_error': 4 failures\n",
      "  Cluster 'ambiguous_input': 4 failures\n",
      "  Cluster 'insufficient_data': 4 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 6 gradients\n",
      "  Generating variants from gradient 1/6\n",
      "  Generating variants from gradient 2/6\n",
      "  Generating variants from gradient 3/6\n",
      "  Generating variants from gradient 4/6\n",
      "  Generating variants from gradient 5/6\n",
      "  Generating variants from gradient 6/6\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To rate the similarity between these two prompts, I\\'ll consider each aspect:\\n\\n1. **Intent**:\\n   Both prompts aim to refine hypotheses or conclusions based on reviewing information and refining them iteratively. They convey similar goals.\\n\\n2. **Constraints**:\\n   The first prompt doesn\\'t specify any particular constraints beyond ensuring all information is reviewed thoroughly. The second prompt mentions \"the complete improved prompt,\" indicating it has additional content not present in the original. This could be seen as a slight constraint but isn\\'t significant enough to differentiate them significantly.\\n\\n3. **Structure**:\\n   The overall structure is very similar; both follow a sequence starting with information gathering and ending with refinement.\\n\\n4. **Examples and Formatting**:\\n   The first prompt provides specific example language for when there\\'s no adequate answer (\"Return \\'No answer\\'\"), which is absent in the second prompt. However, this detail is relatively minor compared to other aspects.\\n\\nConsidering these factors, I would rate the difference as close to 1.0 because they share nearly identical intents, structures, and sequences. The only notable differences are the added complexity of the second prompt and the absence of specific formatting instructions provided in the first one.\\n\\nTherefore, my rating is: **1.0**'\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To rate the similarity between these two prompts, I will consider their intent, constraints, structure, and examples/formatting:\\n\\n### Intent\\n- **A**: Seeking guidance on refining hypotheses based on thorough analysis of all text.\\n- **B**: Emphasizing careful analysis when context is lacking and iteration for clarity.\\n\\n**Similarity**: Both focus on analyzing incomplete contexts and refining hypotheses, so they share some common intent.\\n\\n### Constraints\\n- **A**: No specific constraints mentioned.\\n- **B**: Requires clear and distinct answers only after sufficient iteration.\\n\\n**Similarity**: There\\'s a slight overlap here as both suggest refining until certain criteria are met (incomplete vs. adequate).\\n\\n### Structure\\n- **A**: Uses \"Review the entire text\" and provides examples (\"ensure that no crucial details have been overlooked\").\\n- **B**: Mentions \"If the context lacks sufficient detail\" and uses similar phrasing but specifies \"return \\'No answer\\'\" under specific conditions.\\n\\n**Similarity**: The core message is the same, but the latter explicitly mentions returning \"No answer,\" which adds complexity to the structure.\\n\\n### Examples/Formatting\\n- **A**: Provides concrete actions like reviewing text and refining hypotheses.\\n- **B**: Directly states the condition for not providing an answer and includes an example within its content.\\n\\n**Similarity**: They use slightly different wording but convey the same idea.\\n\\n### Final Rating\\nOn a scale from 0.0 to 1.0, this would likely fall around 0.85. The prompts share substantial similarities while having minor differences due to explicit instructions and added specificity.\\n\\n**Final Score:** 0.85'\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To rate the similarity between these two prompts, let\\'s break down their key elements:\\n\\n### Intent\\n- **Prompt A** seems focused on refining hypotheses based on available data when context lacks sufficient information.\\n- **Prompt B** is incomplete as it only mentions \"The complete improved prompt here,\" without providing specific details or content.\\n\\nIntent-wise, there’s a slight overlap but also significant differences due to the lack of concrete content in Prompt B.\\n\\n### Constraints\\n- Both prompts seem to emphasize systematic examination and addressing gaps to refine hypotheses.\\n- The exact nature of the constraints (e.g., type of data, format) isn\\'t specified, making them somewhat similar but less so because they\\'re missing essential specifics.\\n\\nConstraints: There\\'s a moderate level of similarity due to shared emphasis, but the detailed specifications are lacking.\\n\\n### Structure\\n- **Prompt A**: Structured with bullet points for clarity.\\n- **Prompt B**: Incomplete, leaving room for variation.\\n\\nStructure: They have some similarities due to the use of bullet points for organization, but the overall structure is more open-ended and variable.\\n\\n### Examples and Formatting\\n- **Prompt A**: Uses standard language and bullet points for clarity.\\n- **Prompt B**: Also uses bullet points but leaves space for additional detail that wasn’t provided in the original statement.\\n\\nExamples/Formatting: There\\'s a clear similarity due to the consistent use of bullet points, which helps maintain readability.\\n\\n### Final Rating\\nGiven the analysis above, I would rate this similarity at approximately **0.75**. While both prompts share common themes and structures, the lack of specific content in Prompt B significantly reduces its alignment compared to Prompt A. However, the core focus and basic framework remain very close.'\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To determine the similarity between the two prompts, let\\'s analyze their key components:\\n\\n### Intent:\\nBoth prompts aim to guide someone to make a decision or derive a conclusion based on limited information. The intent is essentially the same.\\n\\n### Constraints:\\n- **Prompt A:** Emphasizes \"adequate information\" and considers \"plausible alternatives.\"\\n- **Prompt B:** Suggests considering all available information thoroughly before making assumptions.\\n\\nThese constraints overlap somewhat but also differ slightly: Prompt A focuses more explicitly on adequacy, while Prompt B emphasizes thorough consideration without specifying adequacy directly.\\n\\n### Structure:\\n- **Prompt A:** Directly states the goal (\"ascertain the ideal response\").\\n- **Prompt B:** Provides a more detailed explanation with additional steps (considering all available information, iterating).\\n\\nThe structure is similar but differs in detail.\\n\\n### Examples and Formatting:\\n- **Prompt A:** Uses straightforward phrasing for clarity.\\n- **Prompt B:** Adds specific instructions about iteration and return value.\\n\\nThese differences are minimal in this case.\\n\\n### Overall Difference:\\nGiven that both prompts convey the same core message—guiding towards a solution based on limited information—they share significant similarities in intent, constraints, and overall structure. However, they diverge slightly in their emphasis and specificity.\\n\\n**Final Rating:**\\n0.95\\n\\nThis rating reflects that while the prompts are very similar in intent and general approach, they have minor differences in their specifics and emphasis.'\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To evaluate the similarity between the two prompts, let\\'s break down their components:\\n\\n**Intent:**\\n- **Prompt A:** Ascertaining an ideal response based on provided context.\\n- **Prompt B:** Exploring the entire text for hidden clues when context is lacking.\\n\\nThe intent seems similar but not exactly the same due to the slight differences in phrasing (\"ideal response\" vs. \"hiding clues\").\\n\\n**Constraints:**\\n- Both prompts emphasize using available context (A) and exploring beyond it (B), indicating they share some constraints related to utilizing available information effectively.\\n\\n**Structure:**\\n- **Prompt A:** Directly asks about ascertaining an ideal response.\\n- **Prompt B:** Encourages exploration and refinement of hypotheses without explicitly stating what to refine them into.\\n\\n**Examples and Formatting:**\\n- **Prompt A:** Uses a specific format with clear instructions.\\n- **Prompt B:** Provides more detailed guidance on refining ideas, which could be interpreted differently depending on interpretation.\\n\\nConsidering all aspects together:\\n\\n- Intent: 0.85\\n- Constraints: 0.9\\n- Structure: 0.7\\n- Examples/Formatting: 0.6\\n\\nOverall similarity rating: **0.75**\\n\\nThis score reflects that while there are significant differences in tone and explicitness, both prompts convey the essence of seeking answers through contextual analysis and exploration.'\n",
      "  Generated 16 variants, 13 unique\n",
      "Generated 13 candidate prompts\n",
      "  Evaluating candidate 1/13... Score: 0.851\n",
      "  Evaluating candidate 2/13... Score: 0.793\n",
      "  Evaluating candidate 3/13... Score: 0.836\n",
      "  Evaluating candidate 4/13... Score: 0.829\n",
      "  Evaluating candidate 5/13... Score: 0.626\n",
      "  Evaluating candidate 6/13... Score: 0.817\n",
      "  Evaluating candidate 7/13... Score: 0.609\n",
      "  Evaluating candidate 8/13... Score: 0.818\n",
      "  Evaluating candidate 9/13... Score: 0.811\n",
      "  Evaluating candidate 10/13... Score: 0.828\n",
      "  Evaluating candidate 11/13... Score: 0.872\n",
      "  Evaluating candidate 12/13... Score: 0.907\n",
      "  Evaluating candidate 13/13... Score: 0.839\n",
      "Evaluated 13 candidates\n",
      "Best candidate score: 0.907 (Δ +0.062)\n",
      "✓ Improvement found! New best: 0.907\n",
      "\n",
      "Validation Set Evaluation:\n",
      "  Validation score: 0.882\n",
      "  Validation accuracy: 0.833\n",
      "  Validation f1: 0.774\n",
      "  Validation robustness: 0.937\n",
      "  Validation efficiency: 0.967\n",
      "  Validation safety: 0.900\n",
      "Iteration time: 7032.51s — LLM calls: 2987 (total: 5708)\n",
      "\n",
      "--- Iteration 2 ---\n",
      "Failures: 11, Successes: 19\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'all': 11 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 2 gradients\n",
      "  Generating variants from gradient 1/2\n",
      "  Generating variants from gradient 2/2\n",
      "  Generated 5 variants, 3 unique\n",
      "Generated 3 candidate prompts\n",
      "  Evaluating candidate 1/3... Score: 0.750\n",
      "  Evaluating candidate 2/3... Score: 0.725\n",
      "  Evaluating candidate 3/3... Score: 0.874\n",
      "Evaluated 3 candidates\n",
      "Best candidate score: 0.874 (Δ -0.033)\n",
      "✗ No significant improvement\n",
      "Iteration time: 1591.51s — LLM calls: 640 (total: 6348)\n",
      "\n",
      "--- Iteration 3 ---\n",
      "Failures: 11, Successes: 19\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'Incorrect Formatting': 11 failures\n",
      "  Cluster 'Missing Context': 1 failures\n",
      "  Cluster 'Logic Error': 11 failures\n",
      "  Cluster 'Unnecessary Information': 1 failures\n",
      "  Cluster 'Ambiguous Language': 1 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 6 gradients\n",
      "  Generating variants from gradient 1/6\n",
      "  Generating variants from gradient 2/6\n",
      "  Generating variants from gradient 3/6\n",
      "  Generating variants from gradient 4/6\n",
      "  Generating variants from gradient 5/6\n",
      "  Generating variants from gradient 6/6\n",
      "  Generated 25 variants, 19 unique\n",
      "Generated 19 candidate prompts\n",
      "  Evaluating candidate 1/19... Score: 0.783\n",
      "  Evaluating candidate 2/19... Score: 0.810\n",
      "  Evaluating candidate 3/19... Score: 0.789\n",
      "  Candidate 4/19: Skipped (already evaluated)\n",
      "  Evaluating candidate 5/19... Score: 0.859\n",
      "  Evaluating candidate 6/19... Score: 0.855\n",
      "  Evaluating candidate 7/19... Score: 0.818\n",
      "  Evaluating candidate 8/19... Score: 0.901\n",
      "  Evaluating candidate 9/19... Score: 0.893\n",
      "  Evaluating candidate 10/19... Score: 0.910\n",
      "  Evaluating candidate 11/19... Score: 0.915\n",
      "  Evaluating candidate 12/19... Score: 0.869\n",
      "  Evaluating candidate 13/19... Score: 0.861\n",
      "  Evaluating candidate 14/19... Score: 0.882\n",
      "  Evaluating candidate 15/19... Score: 0.811\n",
      "  Evaluating candidate 16/19... Score: 0.710\n",
      "  Evaluating candidate 17/19... Score: 0.805\n",
      "  Evaluating candidate 18/19... Score: 0.817\n",
      "  Evaluating candidate 19/19... Score: 0.713\n",
      "Evaluated 18 candidates\n",
      "Best candidate score: 0.915 (Δ +0.008)\n",
      "✓ Improvement found! New best: 0.915\n",
      "\n",
      "Validation Set Evaluation:\n",
      "  Validation score: 0.895\n",
      "  Validation accuracy: 0.833\n",
      "  Validation f1: 0.885\n",
      "  Validation robustness: 0.921\n",
      "  Validation efficiency: 0.933\n",
      "  Validation safety: 0.900\n",
      "Iteration time: 10029.25s — LLM calls: 4154 (total: 10502)\n",
      "\n",
      "============================================================\n",
      "Local Optimization Complete\n",
      "Final score: 0.915\n",
      "Improvements: 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "Phase 2: Global Optimization (Triggered)\n",
      "\n",
      "============================================================\n",
      "GLOBAL OPTIMIZATION STEP | Generation 2\n",
      "============================================================\n",
      "Step 1: Analyzing optimization history...\n",
      "LLM distance evaluation failed, falling back: could not convert string to float: 'To rate the similarity between these two prompts, let\\'s analyze them based on intent, constraints, structure, and examples:\\n\\n### Intent:\\n- **Prompt A**: Seeking clarification about relevance or bias in context and suggesting alternative topics.\\n- **Prompt B**: Requesting extraction of specific factual information and continuing refinement until resolving the question.\\n\\nWhile both prompts aim at seeking clarity or resolution, they have distinct focuses:\\n- Prompt A emphasizes finding issues with the context (irrelevance/misleading).\\n- Prompt B focuses on extracting facts from the context and refining hypotheses.\\n\\n### Constraints:\\nBoth prompts share common constraints such as requiring a response format (\\'Return \"No answer\" if...\\' statements). However, they differ in their expectations for responses:\\n- Prompt A expects a general feedback or suggestion.\\n- Prompt B requires precise factual information and iterative refinement.\\n\\n### Structure:\\nThe structures are quite similar but not exactly the same:\\n- Both prompts use a conditional statement followed by instructions.\\n- The wording differs slightly (\"consider asking a follow-up question\" vs. \"continue refining\").\\n\\n### Examples and Formatting:\\n- Both prompts include example sentences that guide the expected response format.\\n- However, the specifics of what constitutes a valid response differ significantly.\\n\\nGiven these differences, I would rate this similarity score as follows:\\n\\n**Final Rating: 0.6**\\n\\nThis rating reflects the substantial overlap in purpose while acknowledging the key distinctions in focus and requirements.'\n",
      "\n",
      "Step 2: Generating global strategies...\n",
      "Error in global optimization: 'best_score'\n",
      "\n",
      "Phase 3: Population Update\n",
      "\n",
      "  Generation best: 0.915\n",
      "  Overall best: 0.845\n",
      "  ✓ Improvement: +0.070\n",
      "\n",
      "  Generation time: 18696.14s\n",
      "\n",
      "================================================================================\n",
      "GENERATION 3/3\n",
      "================================================================================\n",
      "\n",
      "Phase 1: Local Optimization\n",
      "  Population size: 1\n",
      "\n",
      "  Optimizing node 1/1 (score: 0.915)\n",
      "\n",
      "============================================================\n",
      "Starting Local Optimization\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Iteration 1 ---\n",
      "Failures: 15, Successes: 15\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'all': 15 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 2 gradients\n",
      "  Generating variants from gradient 1/2\n",
      "  Generating variants from gradient 2/2\n",
      "  Generated 5 variants, 5 unique\n",
      "Generated 5 candidate prompts\n",
      "  Evaluating candidate 1/5... Score: 0.772\n",
      "  Evaluating candidate 2/5... Score: 0.852\n",
      "  Evaluating candidate 3/5... Score: 0.847\n",
      "  Evaluating candidate 4/5... Score: 0.732\n",
      "  Evaluating candidate 5/5... Score: 0.892\n",
      "Evaluated 5 candidates\n",
      "Best candidate score: 0.892 (Δ -0.023)\n",
      "✗ No significant improvement\n",
      "Iteration time: 2874.13s — LLM calls: 1065 (total: 11622)\n",
      "\n",
      "--- Iteration 2 ---\n",
      "Failures: 15, Successes: 15\n",
      "Generating text gradients...\n",
      "Clustering failures by error type...\n",
      "  Cluster 'all': 15 failures\n",
      "Generating contrastive gradient...\n",
      "Generated 2 gradients\n",
      "  Generating variants from gradient 1/2\n",
      "  Generating variants from gradient 2/2\n",
      "  Generated 6 variants, 4 unique\n",
      "Generated 4 candidate prompts\n",
      "  Evaluating candidate 1/4... Score: 0.792\n",
      "  Evaluating candidate 2/4... Score: 0.852\n",
      "  Evaluating candidate 3/4... Score: 0.790\n",
      "  Evaluating candidate 4/4... Score: 0.801\n",
      "Evaluated 4 candidates\n",
      "Best candidate score: 0.852 (Δ -0.063)\n",
      "✗ No significant improvement\n",
      "Iteration time: 2314.10s — LLM calls: 854 (total: 12476)\n",
      "\n",
      "Early stopping after 2 iterations without improvement\n",
      "\n",
      "============================================================\n",
      "Local Optimization Complete\n",
      "Final score: 0.915\n",
      "Improvements: 3\n",
      "============================================================\n",
      "\n",
      "\n",
      "Phase 2: Global Optimization (Skipped)\n",
      "\n",
      "Phase 3: Population Update\n",
      "\n",
      "  Generation best: 0.915\n",
      "  Overall best: 0.915\n",
      "  ✗ No significant improvement\n",
      "\n",
      "  Generation time: 5194.72s\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Results:\n",
      "  Initial score: 0.731\n",
      "  Final score: 0.915\n",
      "  Improvement: +0.184 (25.2%)\n",
      "  Total time: 30683.00s\n",
      "  Generations: 3\n",
      "\n",
      "Test Set Evaluation:\n",
      "  Test score: 0.894\n",
      "  Test accuracy: 0.833\n",
      "History saved to ./optimization_results\\optimization_history.json (55 nodes)\n",
      "\n",
      "Results saved to: ./optimization_results\n",
      "  - optimization_history.json\n",
      "  - optimization_report.json\n",
      "  - best_prompt.txt\n",
      "  - trajectory.txt\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_node = optimizer.optimize(\n",
    "    initial_prompt=initial_prompt,\n",
    "    train_examples=train_examples,\n",
    "    validation_examples=validation_examples,\n",
    "    test_examples=test_examples,\n",
    "    save_dir=\"./optimization_results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010feee",
   "metadata": {},
   "source": [
    "## Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ffcd1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:16:29.734754Z",
     "start_time": "2026-01-28T04:16:29.714419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization generations summary:\n",
      "  Generation 1: time 6417.51s, best_score 0.845\n",
      "  Generation 2: time 18696.14s, best_score 0.915\n",
      "  Generation 3: time 5194.72s, best_score 0.915\n",
      "Local optimizer summary:\n",
      "  Total iterations recorded: 8\n",
      "  Avg iteration time: 3781.58s\n",
      "  Total LLM calls attributed to local iterations: 12209\n",
      "Per-iteration breakdown:\n",
      "  Iter 1: time 3254.24s, llm_calls 1441\n",
      "  Iter 2: time 388.29s, llm_calls 5\n",
      "  Iter 3: time 2768.59s, llm_calls 1063\n",
      "  Iter 1: time 7032.51s, llm_calls 2987\n",
      "  Iter 2: time 1591.51s, llm_calls 640\n",
      "  Iter 3: time 10029.25s, llm_calls 4154\n",
      "  Iter 1: time 2874.13s, llm_calls 1065\n",
      "  Iter 2: time 2314.10s, llm_calls 854\n"
     ]
    }
   ],
   "source": [
    "report = optimizer.get_optimization_report()\n",
    "print('Optimization generations summary:')\n",
    "for entry in report['optimization_log']:\n",
    "    print(f\"  Generation {entry['generation']}: time {entry['time']:.2f}s, best_score {entry['best_score']:.3f}\")\n",
    "\n",
    "local_stats = report['component_statistics']['local_optimizer']\n",
    "print('Local optimizer summary:')\n",
    "print(f\"  Total iterations recorded: {local_stats.get('total_iterations', 0)}\")\n",
    "avg_it = local_stats.get('avg_iteration_time')\n",
    "if avg_it is not None:\n",
    "    print(f\"  Avg iteration time: {avg_it:.2f}s\")\n",
    "else:\n",
    "    print('  Avg iteration time: N/A')\n",
    "print(f\"  Total LLM calls attributed to local iterations: {local_stats.get('total_llm_calls_by_local', 0)}\")\n",
    "print('Per-iteration breakdown:')\n",
    "for s in local_stats.get('iteration_stats', []):\n",
    "    print(f\"  Iter {s['iteration']}: time {s['time']:.2f}s, llm_calls {s['llm_calls']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94007335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:16:29.807918Z",
     "start_time": "2026-01-28T04:16:29.800912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PROMPT FOUND:\n",
      "================================================================================\n",
      "Determine the most appropriate response to the inquiry. If insufficient data exists, propose educated guesses or assumptions. In cases where these do not suffice, indicate 'No answer.'\n",
      "================================================================================\n",
      "Score: 0.915\n",
      "Generation: 3\n",
      "Source: local\n"
     ]
    }
   ],
   "source": [
    "print(\"BEST PROMPT FOUND:\")\n",
    "print(\"=\" * 80)\n",
    "print(best_node.prompt_text)\n",
    "print(\"=\" * 80)\n",
    "print(f\"Score: {best_node.metrics.composite_score():.3f}\")\n",
    "print(f\"Generation: {best_node.generation}\")\n",
    "print(f\"Source: {best_node.source.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beea1ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:16:29.984386Z",
     "start_time": "2026-01-28T04:16:29.977183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS:\n",
      "  Composite Score: 0.915\n",
      "  Accuracy:        0.867\n",
      "  Safety:          0.900\n",
      "  Robustness:      0.923\n",
      "  Efficiency:      0.967\n",
      "  F1 Score:        0.917\n"
     ]
    }
   ],
   "source": [
    "metrics = best_node.metrics\n",
    "\n",
    "print(\"METRICS:\")\n",
    "print(f\"  Composite Score: {metrics.composite_score():.3f}\")\n",
    "print(f\"  Accuracy:        {metrics.metrics['accuracy']:.3f}\")\n",
    "print(f\"  Safety:          {metrics.metrics['safety']:.3f}\")\n",
    "print(f\"  Robustness:      {metrics.metrics['robustness']:.3f}\")\n",
    "print(f\"  Efficiency:      {metrics.metrics['efficiency']:.3f}\")\n",
    "print(f\"  F1 Score:        {metrics.metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1698afea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:16:30.055837Z",
     "start_time": "2026-01-28T04:16:30.050359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION TRAJECTORY\n",
      "================================================================================\n",
      "\n",
      "Generation | Best Score | Overall Best | Improvement\n",
      "------------------------------------------------------------\n",
      "   1       | 0.845      | 0.845       | +0.114 ██████████████████████████████████████████\n",
      "   2       | 0.915      | 0.915       | +0.070 █████████████████████████████████████████████\n",
      "   3       | 0.915      | 0.915       | +0.000 █████████████████████████████████████████████\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.visualize_optimization_trajectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ea85b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:16:30.123027Z",
     "start_time": "2026-01-28T04:16:30.111832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZATION REPORT:\n",
      "Overall Statistics:\n",
      "   Total time: 30683.00s\n",
      "   Generations: 3\n",
      "   Total nodes explored: 55\n",
      "Component Statistics:\n",
      "   Local optimizer iterations: 8\n",
      "   Local improvements: 3\n",
      "   Global optimizer steps: 0\n",
      "   Successful global changes: 0\n",
      "Best Global Strategies:\n"
     ]
    }
   ],
   "source": [
    "report = optimizer.get_optimization_report()\n",
    "\n",
    "print(\"OPTIMIZATION REPORT:\")\n",
    "print(\"Overall Statistics:\")\n",
    "print(f\"   Total time: {report['optimization_info']['total_time_seconds']:.2f}s\")\n",
    "print(f\"   Generations: {report['optimization_info']['generations']}\")\n",
    "print(f\"   Total nodes explored: {report['component_statistics']['history']['total_nodes']}\")\n",
    "\n",
    "print(\"Component Statistics:\")\n",
    "print(f\"   Local optimizer iterations: {report['component_statistics']['local_optimizer']['total_iterations']}\")\n",
    "print(f\"   Local improvements: {report['component_statistics']['local_optimizer']['improvements_count']}\")\n",
    "print(f\"   Global optimizer steps: {report['component_statistics']['global_optimizer']['total_global_steps']}\")\n",
    "print(f\"   Successful global changes: {report['component_statistics']['global_optimizer']['successful_global_changes']}\")\n",
    "\n",
    "print(\"Best Global Strategies:\")\n",
    "for i, strategy in enumerate(report['best_global_strategies'][:3], 1):\n",
    "    print(f\"   {i}. {strategy['strategy']['type']}: Score {strategy['score']:.3f}\")\n",
    "    print(f\"      {strategy['strategy']['description'][:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efddb88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:16:30.197351Z",
     "start_time": "2026-01-28T04:16:30.189566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVOLUTION OF BEST PROMPT:\n",
      "================================================================================\n",
      "Step 0: Generation 0, Source: initial\n",
      "  Score: 0.731\n",
      "  ↓\n",
      "Step 1: Generation 1, Source: local\n",
      "  Score: 0.845\n",
      "  Operations:\n",
      "    - modify_instruction: Edited based on gradient...\n",
      "  ↓\n",
      "Step 2: Generation 2, Source: local\n",
      "  Score: 0.907\n",
      "  Operations:\n",
      "    - modify_instruction: Edited based on gradient...\n",
      "  ↓\n",
      "Step 3: Generation 3, Source: local\n",
      "  Score: 0.915\n",
      "  Operations:\n",
      "    - modify_instruction: Edited based on gradient...\n"
     ]
    }
   ],
   "source": [
    "lineage = optimizer.history.get_lineage(best_node.id)\n",
    "\n",
    "print(\"EVOLUTION OF BEST PROMPT:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, node in enumerate(lineage):\n",
    "    print(f\"Step {i}: Generation {node.generation}, Source: {node.source.value}\")\n",
    "    if node.is_evaluated:\n",
    "        print(f\"  Score: {node.metrics.composite_score():.3f}\")\n",
    "\n",
    "    if node.operations:\n",
    "        print(f\"  Operations:\")\n",
    "        for op in node.operations:\n",
    "            print(f\"    - {op.operation_type.value}: {op.description[:60]}...\")\n",
    "\n",
    "    if i < len(lineage) - 1:  \n",
    "        print(\"  ↓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adbf09aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:30:55.546881Z",
     "start_time": "2026-01-28T04:16:30.231730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing with baseline...\n",
      "\n",
      "Comparison Results:\n",
      "  Baseline score: 0.704\n",
      "  Optimized score: 0.889\n",
      "  Improvement: +0.185\n",
      "COMPARISON WITH BASELINE:\n",
      "================================================================================\n",
      "Baseline:\n",
      "  composite_score     : 0.704\n",
      "  accuracy            : 0.467\n",
      "  safety              : 0.633\n",
      "  robustness          : 0.893\n",
      "  efficiency          : 0.800\n",
      "  f1                  : 0.727\n",
      "Optimized:\n",
      "  composite_score     : 0.889\n",
      "  accuracy            : 0.833\n",
      "  safety              : 0.900\n",
      "  robustness          : 0.916\n",
      "  efficiency          : 0.900\n",
      "  f1                  : 0.895\n",
      "Improvements:\n",
      "  composite_score     : +0.185 ↑\n",
      "  accuracy            : +0.367 ↑\n",
      "  safety              : +0.267 ↑\n",
      "  robustness          : +0.023 ↑\n",
      "  efficiency          : +0.100 ↑\n",
      "  f1                  : +0.168 ↑\n"
     ]
    }
   ],
   "source": [
    "comparison = optimizer.compare_with_baseline(\n",
    "    baseline_prompt=initial_prompt,\n",
    "    test_examples=test_examples\n",
    ")\n",
    "\n",
    "print(\"COMPARISON WITH BASELINE:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Baseline:\")\n",
    "for metric, value in comparison['baseline'].items():\n",
    "    print(f\"  {metric:20s}: {value:.3f}\")\n",
    "\n",
    "print(\"Optimized:\")\n",
    "for metric, value in comparison['optimized'].items():\n",
    "    print(f\"  {metric:20s}: {value:.3f}\")\n",
    "\n",
    "print(\"Improvements:\")\n",
    "for metric, value in comparison['improvements'].items():\n",
    "    arrow = \"↑\" if value > 0 else \"↓\" if value < 0 else \"→\"\n",
    "    print(f\"  {metric:20s}: {value:+.3f} {arrow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb5d4299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T04:30:55.677666Z",
     "start_time": "2026-01-28T04:30:55.669131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "Overall Statistics:\n",
      "  Total nodes explored: 55\n",
      "  Evaluations performed: 55\n",
      "  Generations completed: 4\n",
      "  Best score achieved: 0.915\n",
      "  Average score: 0.788\n",
      "Local Optimization:\n",
      "  Total iterations: 8\n",
      "  Improvements found: 3\n",
      "  Success rate: 37.5%\n",
      "Global Optimization:\n",
      "  Total global steps: 0\n",
      "  Candidates generated: 0\n",
      "  Successful changes: 0\n",
      "  Success rate: 0.0%\n",
      "Optimization complete!\n",
      "   Results saved to: ./optimization_results/\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "history_stats = optimizer.history.get_statistics()\n",
    "local_stats = optimizer.local_optimizer.get_statistics()\n",
    "global_stats = optimizer.global_optimizer.get_statistics()\n",
    "\n",
    "print(\"Overall Statistics:\")\n",
    "print(f\"  Total nodes explored: {history_stats['total_nodes']}\")\n",
    "print(f\"  Evaluations performed: {history_stats['evaluated_nodes']}\")\n",
    "print(f\"  Generations completed: {history_stats['max_generation']}\")\n",
    "print(f\"  Best score achieved: {history_stats['best_score']:.3f}\")\n",
    "print(f\"  Average score: {history_stats['avg_score']:.3f}\")\n",
    "\n",
    "print(\"Local Optimization:\")\n",
    "print(f\"  Total iterations: {local_stats['total_iterations']}\")\n",
    "print(f\"  Improvements found: {local_stats['improvements_count']}\")\n",
    "print(f\"  Success rate: {local_stats['improvement_rate']:.1%}\")\n",
    "\n",
    "print(\"Global Optimization:\")\n",
    "print(f\"  Total global steps: {global_stats['total_global_steps']}\")\n",
    "print(f\"  Candidates generated: {global_stats['total_candidates_generated']}\")\n",
    "print(f\"  Successful changes: {global_stats['successful_global_changes']}\")\n",
    "print(f\"  Success rate: {global_stats['success_rate']:.1%}\")\n",
    "\n",
    "print(\"Optimization complete!\")\n",
    "print(f\"   Results saved to: ./optimization_results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
